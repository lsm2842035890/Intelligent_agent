Failure # 1 (occurred at 2024-07-02_16-19-39)
Traceback (most recent call last):
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\tune\trial_runner.py", line 886, in _process_trial
    results = self.trial_executor.fetch_result(trial)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\tune\ray_trial_executor.py", line 675, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\_private\client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\worker.py", line 1765, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::PPOTrainer.__init__()[39m (pid=16752, ip=127.0.0.1, repr=PPOTrainer)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\util\tracing\tracing_helper.py", line 451, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\rllib\agents\trainer.py", line 925, in _init
    raise NotImplementedError
NotImplementedError

During handling of the above exception, another exception occurred:

[36mray::PPOTrainer.__init__()[39m (pid=16752, ip=127.0.0.1, repr=PPOTrainer)
  File "python\ray\_raylet.pyx", line 633, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 674, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 644, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 593, in ray._raylet.execute_task.function_executor
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\_private\function_manager.py", line 648, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\util\tracing\tracing_helper.py", line 451, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\rllib\agents\trainer.py", line 746, in __init__
    super().__init__(config, logger_creator, remote_checkpoint_dir,
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\tune\trainable.py", line 124, in __init__
    self.setup(copy.deepcopy(self.config))
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\util\tracing\tracing_helper.py", line 451, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\rllib\agents\trainer.py", line 822, in setup
    self.workers = self._make_workers(
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\util\tracing\tracing_helper.py", line 451, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\rllib\agents\trainer.py", line 1995, in _make_workers
    return WorkerSet(
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\rllib\evaluation\worker_set.py", line 101, in __init__
    remote_spaces = ray.get(self.remote_workers(
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\_private\client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\worker.py", line 1765, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=14944, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000244FE967970>)
  File "python\ray\_raylet.pyx", line 600, in ray._raylet.execute_task
  File "C:\Users\28420\AppData\Roaming\Python\Python38\site-packages\ray\_private\memory_monitor.py", line 156, in raise_if_low_memory
    raise RayOutOfMemoryError(
ray._private.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node DESKTOP-GIIT3U4 is used (7.82 / 7.84 GB). The top 10 memory consumers are:

PID	MEM	COMMAND
7984	0.48GiB	E:\Microsoft VS Code\Code.exe c:\Users\28420\.vscode\extensions\ms-python.vscode-pylance-2024.6.1\di
8600	0.36GiB	E:\Microsoft VS Code\Code.exe --type=renderer --user-data-dir=C:\Users\28420\AppData\Roaming\Code --
19908	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe c:/Users/28420/Desktop/SAIC/Dense-Deep-Reinforc
16752	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\
14944	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\
17480	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\
19024	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\
15932	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\
17736	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\
5744	0.25GiB	C:\Users\28420\.conda\envs\d2rl_train_new\python.exe C:\Users\28420\AppData\Roaming\Python\Python38\

In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store.
---
--- Tip: Use the `ray memory` command to list active objects in the cluster.
--- To disable OOM exceptions, set RAY_DISABLE_MEMORY_MONITOR=1.
---

